# configs/model/vit/vit_base.yaml
defaults:
  - ../transformer/base_transformer
  # - _self_

name: vit_base
type: vision_transformer
# architecture: vit # Inherited from base_transformer as 'transformer'

# Model parameters
params:
  img_size: 256 # Overrides base_transformer's 224
  # patch_size: 16 # Inherited from base_transformer
  in_chans: 1  # Grayscale
  # num_classes: 2 # Inherited from base
  embed_dim: 768  # Base dimension
  depth: 12
  num_heads: 12
  mlp_ratio: 4.0
  qkv_bias: true
  representation_size: null
  drop_rate: 0.1
  attn_drop_rate: 0.0
  drop_path_rate: 0.1
  class_token: true
  pos_embed_type: learnable
  pool_type: cls
  
# No pretrained weights for standard ViT
# pretrained: false # Overrides base.yaml's true, and base_transformer inherits from base.

# Model initialization
init:
  mode: trunc_normal
  std: 0.02
  
# Quality-aware preprocessing
quality_aware: true

# Expected parameter count: ~86M