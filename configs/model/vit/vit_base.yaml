# configs/model/vit/vit_base.yaml
name: vit_base
type: vision_transformer
architecture: vit

# Model parameters
params:
  img_size: 256
  patch_size: 16
  in_chans: 1  # Grayscale
  num_classes: 2
  embed_dim: 768  # Base dimension
  depth: 12
  num_heads: 12
  mlp_ratio: 4.0
  qkv_bias: true
  representation_size: null
  drop_rate: 0.1
  attn_drop_rate: 0.0
  drop_path_rate: 0.1
  class_token: true
  pos_embed_type: learnable
  pool_type: cls
  
# No pretrained weights for standard ViT
pretrained: false

# Model initialization
init:
  mode: trunc_normal
  std: 0.02
  
# Quality-aware preprocessing
quality_aware: true

# Expected parameter count: ~86M