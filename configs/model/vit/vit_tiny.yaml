# configs/model/vit/vit_tiny.yaml
defaults:
  - ../transformer/base_transformer
  # - _self_

name: vit_tiny
type: vision_transformer
# architecture: vit # Inherited from base_transformer as 'transformer'

# Model parameters
params:
  img_size: 256 # Overrides base_transformer's 224
  # patch_size: 16 # Inherited from base_transformer
  in_chans: 1  # Grayscale thyroid images
  # num_classes: 2  # Inherited from base
  embed_dim: 192  # Tiny model dimension
  depth: 12
  num_heads: 3  # 192 / 64 = 3
  mlp_ratio: 4.0
  qkv_bias: true
  representation_size: null
  drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.1  # Stochastic depth
  embed_layer: PatchEmbed
  norm_layer: LayerNorm
  act_layer: GELU
  weight_init: trunc_normal
  class_token: true
  pos_embed_type: learnable
  pool_type: cls
  
# No pretrained weights for standard ViT
# pretrained: false # Overrides base.yaml's true, and base_transformer inherits from base.
  
# Attention visualization
attention:
  save_attention_maps: true
  visualize_layers: [11]  # Last layer
  
# Model initialization
init:
  mode: trunc_normal
  std: 0.02
  
# Quality-aware preprocessing
quality_aware: true

# Expected parameter count: ~5.7M