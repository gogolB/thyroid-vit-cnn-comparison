# configs/model/vit/vit_tiny.yaml
name: vit_tiny
type: vision_transformer
architecture: vit

# Model parameters
params:
  img_size: 256
  patch_size: 16
  in_chans: 1  # Grayscale thyroid images
  num_classes: 2  # Binary classification
  embed_dim: 192
  depth: 12
  num_heads: 3
  mlp_ratio: 4.0
  qkv_bias: true
  representation_size: null
  distilled: false
  drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.1  # Stochastic depth
  embed_layer: PatchEmbed
  norm_layer: LayerNorm
  act_layer: GELU
  weight_init: trunc_normal
  class_token: true
  pos_embed_type: learnable
  pool_type: cls  # 'cls' or 'gap'
  
# Attention visualization settings
attention:
  save_attention_maps: true
  attention_probs_dropout_prob: 0.0
  
# Model initialization
init:
  mode: trunc_normal
  std: 0.02
  
# Quality-aware preprocessing (inherited from CNN success)
quality_aware: true

# Expected parameter count: ~5.7M
