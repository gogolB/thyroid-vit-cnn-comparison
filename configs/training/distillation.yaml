# configs/training/distillation.yaml
# Knowledge distillation training configuration for Vision Transformers

# Inherit from standard ViT training but override specific parameters
defaults:
  - vit_standard

# Training modifications for distillation
batch_size: 32  # May need to reduce if using large teacher
num_epochs: 300

# Optimizer adjustments for distillation
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005  # Lower learning rate for distillation
  weight_decay: 0.05
  betas: [0.9, 0.999]

# Scheduler with longer warmup
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 300
  eta_min: 1e-6
  
# Add warmup configuration
warmup_epochs: 20  # Longer warmup for stable distillation

# Loss configuration (base classification loss)
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1  # Can combine with distillation

# Monitoring
monitor_metrics:
  - train_loss
  - train_acc
  - val_acc
  - distill_loss  # Distillation component
  - class_loss  # Classification component
  - teacher_agreement  # How often student matches teacher

# Early stopping based on validation accuracy
early_stopping:
  patience: 30
  min_delta: 0.001
  mode: max
  monitor: val_acc

# Gradient clipping for stability
gradient_clip_val: 1.0

# Mixed precision training
precision: "16-mixed"

# Logging
log_every_n_steps: 10
val_check_interval: 1.0

# Checkpointing
checkpoint:
  save_top_k: 3
  monitor: val_acc
  mode: max
  save_last: true
  filename: "{epoch:02d}-{val_acc:.4f}-{distill_loss:.4f}"