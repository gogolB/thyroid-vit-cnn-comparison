# EfficientNet-specific training configuration
batch_size: 32  # May need to reduce for B3
num_epochs: 150  # More epochs for better convergence

# Optimizer configuration - RMSprop often works better with EfficientNet
optimizer:
  _target_: torch.optim.RMSprop
  lr: ${model.learning_rate}  # Model-specific learning rate
  alpha: 0.9
  eps: 0.001
  momentum: 0.9
  weight_decay: ${model.weight_decay}

# Alternative: AdamW optimizer (comment out RMSprop above and uncomment below)
# optimizer:
#   _target_: torch.optim.AdamW
#   lr: ${model.learning_rate}
#   betas: [0.9, 0.999]
#   eps: 1e-08
#   weight_decay: ${model.weight_decay}

# Learning rate scheduler - Exponential decay works well with EfficientNet
scheduler:
  _target_: torch.optim.lr_scheduler.ExponentialLR
  gamma: 0.97  # Decay factor per epoch

# Alternative: Cosine annealing with warm restarts
# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#   T_0: 10  # Initial restart period
#   T_mult: 2  # Factor to increase period after restart
#   eta_min: 1e-6

# Loss function with label smoothing
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1  # EfficientNet benefits from label smoothing

# Early stopping configuration
early_stopping:
  patience: 20  # More patience for EfficientNet
  min_delta: 0.001
  mode: max
  monitor: val/acc

# Gradient clipping
gradient_clip_val: 1.0

# Gradient accumulation (useful for larger models with memory constraints)
accumulate_grad_batches: 1  # Increase to 2 or 4 for B3 if needed

# Mixed precision training
precision: "16-mixed"  # Use 32 for MPS

# Deterministic training
deterministic: false

# EfficientNet-specific augmentation level
augmentation_level: medium  # Can be increased to 'heavy' for better regularization

# Warmup epochs (optional but recommended)
warmup_epochs: 5
warmup_lr_scale: 0.1  # Start with 10% of target LR

# Stochastic depth (survival probability)
# Already handled by drop_connect_rate in model config
# stochastic_depth_prob: 0.2  # For reference

# Additional regularization
# mixup_alpha: 0.2  # Uncomment to enable mixup
# cutmix_alpha: 1.0  # Uncomment to enable cutmix