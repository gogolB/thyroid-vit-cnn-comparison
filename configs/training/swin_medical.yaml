# configs/training/swin_medical.yaml
# Swin Transformer optimized for medical imaging
name: swin_medical
batch_size: 16  # Swin uses more memory
num_epochs: 300
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.05
  betas: [0.9, 0.999]
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 300
  eta_min: 1e-7
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1
early_stopping:
  patience: 30
  min_delta: 0.001
  mode: max
  monitor: val_acc
gradient_clip_val: 1.0
accumulate_grad_batches: 2
precision: "16-mixed"

# Swin-specific augmentation
augmentation:
  randaugment:
    n: 2
    m: 12  # Slightly less than maximum
  mixup_alpha: 0.2  # Less aggressive for Swin
  cutmix_alpha: 1.0
  cutmix_prob: 0.3
  use_quality_aware: true
  quality_threshold: 0.65
  patch_drop_prob: 0.05  # Conservative

# Swin architecture settings
model:
  window_size: 7  # Standard for Swin
  patch_size: 4  # Swin uses smaller patches
  embed_dim: 96
  depths: [2, 2, 6, 2]
  num_heads: [3, 6, 12, 24]
  
# Pretrained settings
pretrained: true
pretrained_type: imagenet
pretrained_path: "microsoft/swin-tiny-patch4-window7-224"

# Layer-wise LR decay (important for Swin)
layer_wise_lr_decay:
  enabled: true
  decay_rate: 0.9  # Less aggressive than ViT
