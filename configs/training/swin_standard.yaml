# Standard training configuration for Swin Transformer
# Optimized for hierarchical vision transformers

# Basic training parameters
batch_size: 32
num_epochs: 300
gradient_clip_val: 5.0  # Added for trainer compatibility
accumulate_grad_batches: 1  # Added for trainer compatibility
deterministic: false  # Added for trainer compatibility
precision: "16-mixed"  # Added for consistency

# Optimizer configuration (matches other configs structure)
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001
  weight_decay: 0.05
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler configuration (matches other configs structure)
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 300
  eta_min: 1e-5

# Loss function with label smoothing
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1

# Early stopping configuration
early_stopping:
  patience: 50
  min_delta: 0.001
  mode: max
  monitor: val_acc
  verbose: true

# Warmup configuration
warmup:
  warmup_epochs: 20
  warmup_method: linear
  warmup_factor: 0.001

# Layer-wise learning rate decay
layer_decay:
  enabled: true
  decay_rate: 0.85

# Data loading
num_workers: 8
pin_memory: true
persistent_workers: true

# Swin-specific settings (kept for model reference)
swin_specific:
  # Original gradient clipping settings
  gradient_clip:
    enabled: true
    max_norm: 5.0
    norm_type: 2
    
  # Stochastic depth (drop path)
  drop_path:
    enabled: true
    rate: 0.2
    mode: "linear"
    
  # EMA (Exponential Moving Average)
  ema:
    enabled: true
    decay: 0.9999
    update_every: 1

# Augmentation configuration (simplified to match expected structure)
augmentation:
  # RandAugment settings
  randaugment:
    enabled: true
    n: 2
    m: 9
    
  # MixUp settings
  mixup:
    enabled: true
    alpha: 0.8
    prob: 0.5
    switch_prob: 0.5
    mode: batch
    
  # CutMix settings
  cutmix:
    enabled: true
    alpha: 1.0
    prob: 0.5
    
  # Standard augmentations
  random_crop: true
  random_flip: true
  color_jitter: false  # Disabled for grayscale

# Regularization
regularization:
  drop_path_rate: 0.2
  dropout_rate: 0.0
  attention_dropout: 0.0

# Monitoring and logging
log_every_n_steps: 10
val_check_interval: 1.0
save_top_k: 3
save_last: true

# Dataset specific settings
dataset_specific:
  class_weights: null
  quality_aware_sampling: true
  oversample_high_quality: 1.5